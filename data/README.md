# ğŸ“Š Matrix Methods in Machine Learning

This repository contains a collection of **machine learning practices** using matrix-based methods. The goal is to explore how matrix operations and optimization techniques are used in **classification, regression, dimensionality reduction, and deep learning**. Each notebook showcases an important aspect of **Matrix Methods in Machine Learning**, providing insights and practical implementations.

## ğŸ“‚ Practice Highlights

Below are the featured practices in this portfolio:

### ğŸ“˜ **[00 matrix functions](./00_matrix_functions.ipynb)**

### ğŸ“˜ **[e) Describe the decision boundary you observe using a sentence: The decision boundary appears to pass through point1 at (0, 0.2) and point2 at (1, 0.7), suggesting that the boundary equation could be represented as x_2 = 0.5 * x_1 + 0.2.](./01_linear_classiï¬er.ipynb)**

![](/mnt/data/01_linear_classiï¬er.png)

![](/mnt/data/01_linear_classiï¬er.png)

### ğŸ“˜ **[### 1. a) Are the columns of the following matrix linearly independent?](./02_linearly_independent.ipynb)**

### ğŸ“˜ **[# 1. Polynomial ï¬tting](./03_polynomial_ï¬tting.ipynb)**

![](/mnt/data/03_polynomial_ï¬tting.png)

![](/mnt/data/03_polynomial_ï¬tting.png)

![](/mnt/data/03_polynomial_ï¬tting.png)

### ğŸ“˜ **[### Gram-Schmidt orthogonalization](./04_movies_rating.ipynb)**

### ğŸ“˜ **[# 1. Happy/ Angry Classifier](./05_happy_angry_classifier.ipynb)**

### ğŸ“˜ **[#### <font color='blue'>2a) Does the data appear to lie in a low-dimensional subspace? Why or why not? Remember the deï¬nition of a subspace.</font>](./06_SVD.ipynb)**

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

### ğŸ“˜ **[#### 2a) For a binary linear classiï¬er, explain (mathematically) why the logistic loss function does not suï¬€er from the same problem as the squared error loss on easy to classify point.](./07_logistic_regression.ipynb)**

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

### ğŸ“˜ **[### 1a) ](./08_LASSO.ipynb)**

![](/mnt/data/08_LASSO.png)

![](/mnt/data/08_LASSO.png)

![](/mnt/data/08_LASSO.png)

![](/mnt/data/08_LASSO.png)

### ğŸ“˜ **[Kernel Classification Example](./09_kernel_classification.ipynb)**

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

### ğŸ“˜ **[#### 1a) Run the regression script with Ïƒ = 0.04 and Î» = 0.01. Figure 1 displays several of the kernels K(x, xi). What is the value xi associated with the kernel having the third peak from the left? What property of the kernel is determined by xi? What property is determined by Ïƒ?](./10_kernel_regression.ipynb)**

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

### ğŸ“˜ **[## Add correct points far from boundary](./11_linear_svm.ipynb)**

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

### ğŸ“˜ **[# f) Of the three classifiers, which one performs worse? Why?](./12_linear_classifier_overfitting.ipynb)**

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

### ğŸ“˜ **[## CS/ECE/ME532 Period 10 Activity](./13_Kmeans_SVD.ipynb)**

![](/mnt/data/13_Kmeans_SVD.png)

![](/mnt/data/13_Kmeans_SVD.png)

![](/mnt/data/13_Kmeans_SVD.png)

### ğŸ“˜ **[## CS/ECE/ME 532 -  Activity 11 Item 1](./13_Kmeans_SVD02.ipynb)**

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

### ğŸ“˜ **[### a) Run the code to display the data in Figure the ï¬rst ï¬gure. Use the rotate tool to inspect the scatter plot from diï¬€erent angles. Does the data appear to lie very close to a one-dimensional subspace? Does the data appear to be zero mean?](./14_PCA.ipynb)**

![](/mnt/data/14_PCA.png)

![](/mnt/data/14_PCA.png)

![](/mnt/data/14_PCA.png)

### ğŸ“˜ **[15 eigenfaces](./15_eigenfaces.ipynb)**

![](/mnt/data/15_eigenfaces.png)

![](/mnt/data/15_eigenfaces.png)

### ğŸ“˜ **[### What do you conclude about the approximate rank of A? Why is it useful to plot the logarithm of the singular values?](./16_bucky.ipynb)**

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

### ğŸ“˜ **[17 color image compression](./17_color_image_compression.ipynb)**

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

### ğŸ“˜ **[#### <font color='blue'>a) Apply the iterative singular value thresholding function (provided in the script) to the three incomplete matrices assuming the rank is 2. You will ï¬rst need to complete the line of code in the function. Compare your recovered completed matrices to Xtrue (Note: compare the output by subtracting the completed matrix from the original matrix, and then displaying them). Does the number of missing entries aï¬€ect the accuracy of the completed matrix?</font>
](./18_matrix_completion.ipynb)**

### ğŸ“˜ **[## Neural network example](./19_neural_net.ipynb)**

![](/mnt/data/19_neural_net.png)

![](/mnt/data/19_neural_net.png)

![](/mnt/data/19_neural_net.png)

![](/mnt/data/19_neural_net.png)

### ğŸ“˜ **[---](./20_proximal_gradient_descent.ipynb)**

![](/mnt/data/20_proximal_gradient_descent.png)

![](/mnt/data/20_proximal_gradient_descent.png)

### ğŸ“˜ **[---](./21_proximal_gradient_descent_l1.ipynb)**

![](/mnt/data/21_proximal_gradient_descent_l1.png)

![](/mnt/data/21_proximal_gradient_descent_l1.png)

### ğŸ“˜ **[## Two neuron example](./22_two_neuron.ipynb)**

![](/mnt/data/22_two_neuron.png)

![](/mnt/data/22_two_neuron.png)

![](/mnt/data/22_two_neuron.png)

![](/mnt/data/22_two_neuron.png)

## ğŸ”¥ Skills Demonstrated
- **Kernel Methods & Support Vector Machines (SVM)**
- **LASSO Regression**
- **Matrix Methods in Machine Learning**
- **Optimization with Gradient Descent**
- **Principal Component Analysis (PCA)**
- **Singular Value Decomposition (SVD) & Eigenfaces**


---

*This portfolio showcases my ability to implement advanced matrix-based techniques in machine learning. Thank you for visiting!*
