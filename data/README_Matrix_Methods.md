# ğŸ“Š Matrix Methods in Machine Learning

This repository contains a collection of **machine learning practices** using matrix-based methods. The goal is to explore how matrix operations and optimization techniques are used in **classification, regression, dimensionality reduction, and deep learning**. Each notebook showcases an important aspect of **Matrix Methods in Machine Learning**, providing insights and practical implementations.

## ğŸ“‚ Practice Highlights

Below are the featured practices in this portfolio, along with the machine learning methods applied and problems addressed:

### ğŸ“˜ **[00 matrix functions](./00_matrix_functions.ipynb)**

**Machine Learning Method Used:** Basic matrix operations for machine learning applications.

**Problem Addressed:** Understanding fundamental matrix functions for data manipulation and transformation.

### ğŸ“˜ **[e) Describe the decision boundary you observe using a sentence: The decision boundary appears to pass through point1 at (0, 0.2) and point2 at (1, 0.7), suggesting that the boundary equation could be represented as x_2 = 0.5 * x_1 + 0.2.](./01_linear_classiï¬er.ipynb)**

**Machine Learning Method Used:** Linear classification using decision boundaries.

**Problem Addressed:** Classifying data points into two categories using a linear decision boundary.

![](/mnt/data/01_linear_classiï¬er.png)

![](/mnt/data/01_linear_classiï¬er.png)

### ğŸ“˜ **[### 1. a) Are the columns of the following matrix linearly independent?](./02_linearly_independent.ipynb)**

**Machine Learning Method Used:** Identifying linearly independent features for dimensionality reduction.

**Problem Addressed:** Determining which features in a dataset contribute unique information to improve model efficiency.

### ğŸ“˜ **[# 1. Polynomial ï¬tting](./03_polynomial_ï¬tting.ipynb)**

**Machine Learning Method Used:** Polynomial regression for curve fitting.

**Problem Addressed:** Fitting a polynomial function to data points to capture nonlinear relationships.

![](/mnt/data/03_polynomial_ï¬tting.png)

![](/mnt/data/03_polynomial_ï¬tting.png)

![](/mnt/data/03_polynomial_ï¬tting.png)

### ğŸ“˜ **[### Gram-Schmidt orthogonalization](./04_movies_rating.ipynb)**

**Machine Learning Method Used:** Matrix factorization for collaborative filtering.

**Problem Addressed:** Predicting user preferences in a movie rating system using latent factor models.

### ğŸ“˜ **[# 1. Happy/ Angry Classifier](./05_happy_angry_classifier.ipynb)**

**Machine Learning Method Used:** Binary classification using facial emotion recognition.

**Problem Addressed:** Classifying images of faces into 'happy' or 'angry' emotions using machine learning.

### ğŸ“˜ **[#### <font color='blue'>2a) Does the data appear to lie in a low-dimensional subspace? Why or why not? Remember the deï¬nition of a subspace.</font>](./06_SVD.ipynb)**

**Machine Learning Method Used:** Singular Value Decomposition (SVD) for feature extraction.

**Problem Addressed:** Reducing dimensionality while retaining essential data information for efficient computations.

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

![](/mnt/data/06_SVD.png)

### ğŸ“˜ **[#### 2a) For a binary linear classiï¬er, explain (mathematically) why the logistic loss function does not suï¬€er from the same problem as the squared error loss on easy to classify point.](./07_logistic_regression.ipynb)**

**Machine Learning Method Used:** Logistic regression for binary classification.

**Problem Addressed:** Predicting binary outcomes based on input features, such as spam detection or disease prediction.

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

![](/mnt/data/07_logistic_regression.png)

### ğŸ“˜ **[### 1a) ](./08_LASSO.ipynb)**

**Machine Learning Method Used:** LASSO regression for feature selection and regularization.

**Problem Addressed:** Preventing overfitting while selecting the most important predictors in a dataset.

![](/mnt/data/08_LASSO.png)

![](/mnt/data/08_LASSO.png)

![](/mnt/data/08_LASSO.png)

![](/mnt/data/08_LASSO.png)

### ğŸ“˜ **[Kernel Classification Example](./09_kernel_classification.ipynb)**

**Machine Learning Method Used:** Kernel methods for nonlinear classification.

**Problem Addressed:** Classifying data with complex decision boundaries that are not separable using linear models.

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

![](/mnt/data/09_kernel_classification.png)

### ğŸ“˜ **[#### 1a) Run the regression script with Ïƒ = 0.04 and Î» = 0.01. Figure 1 displays several of the kernels K(x, xi). What is the value xi associated with the kernel having the third peak from the left? What property of the kernel is determined by xi? What property is determined by Ïƒ?](./10_kernel_regression.ipynb)**

**Machine Learning Method Used:** Kernel regression for function approximation.

**Problem Addressed:** Predicting continuous values using flexible nonlinear regression techniques.

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

![](/mnt/data/10_kernel_regression.png)

### ğŸ“˜ **[## Add correct points far from boundary](./11_linear_svm.ipynb)**

**Machine Learning Method Used:** Support Vector Machine (SVM) for classification.

**Problem Addressed:** Finding the optimal hyperplane to separate classes with maximum margin.

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

![](/mnt/data/11_linear_svm.png)

### ğŸ“˜ **[# f) Of the three classifiers, which one performs worse? Why?](./12_linear_classifier_overfitting.ipynb)**

**Machine Learning Method Used:** Overfitting analysis in linear classification.

**Problem Addressed:** Understanding and mitigating overfitting in machine learning models.

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

![](/mnt/data/12_linear_classifier_overfitting.png)

### ğŸ“˜ **[## CS/ECE/ME532 Period 10 Activity](./13_Kmeans_SVD.ipynb)**

**Machine Learning Method Used:** K-Means clustering combined with SVD for feature extraction.

**Problem Addressed:** Grouping similar data points into clusters while reducing feature dimensions.

![](/mnt/data/13_Kmeans_SVD.png)

![](/mnt/data/13_Kmeans_SVD.png)

![](/mnt/data/13_Kmeans_SVD.png)

### ğŸ“˜ **[## CS/ECE/ME 532 -  Activity 11 Item 1](./13_Kmeans_SVD02.ipynb)**

**Machine Learning Method Used:** Advanced K-Means clustering with SVD.

**Problem Addressed:** Exploring refined clustering techniques to enhance data grouping accuracy.

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

![](/mnt/data/13_Kmeans_SVD02.png)

### ğŸ“˜ **[### a) Run the code to display the data in Figure the ï¬rst ï¬gure. Use the rotate tool to inspect the scatter plot from diï¬€erent angles. Does the data appear to lie very close to a one-dimensional subspace? Does the data appear to be zero mean?](./14_PCA.ipynb)**

**Machine Learning Method Used:** Principal Component Analysis (PCA) for dimensionality reduction.

**Problem Addressed:** Reducing high-dimensional data while preserving the most significant variance.

![](/mnt/data/14_PCA.png)

![](/mnt/data/14_PCA.png)

![](/mnt/data/14_PCA.png)

### ğŸ“˜ **[15 eigenfaces](./15_eigenfaces.ipynb)**

**Machine Learning Method Used:** Eigenfaces approach for facial recognition.

**Problem Addressed:** Identifying and recognizing faces using a reduced set of eigenvectors.

![](/mnt/data/15_eigenfaces.png)

![](/mnt/data/15_eigenfaces.png)

### ğŸ“˜ **[### What do you conclude about the approximate rank of A? Why is it useful to plot the logarithm of the singular values?](./16_bucky.ipynb)**

**Machine Learning Method Used:** Graph-based optimization in machine learning.

**Problem Addressed:** Optimizing node connectivity and network efficiency using graph-based representations.

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

![](/mnt/data/16_bucky.png)

### ğŸ“˜ **[17 color image compression](./17_color_image_compression.ipynb)**

**Machine Learning Method Used:** Color image compression using matrix factorization.

**Problem Addressed:** Reducing the storage size of color images while maintaining visual quality.

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

![](/mnt/data/17_color_image_compression.png)

### ğŸ“˜ **[#### <font color='blue'>a) Apply the iterative singular value thresholding function (provided in the script) to the three incomplete matrices assuming the rank is 2. You will ï¬rst need to complete the line of code in the function. Compare your recovered completed matrices to Xtrue (Note: compare the output by subtracting the completed matrix from the original matrix, and then displaying them). Does the number of missing entries aï¬€ect the accuracy of the completed matrix?</font>](./18_matrix_completion.ipynb)**

**Machine Learning Method Used:** Matrix completion techniques for missing data imputation.

**Problem Addressed:** Predicting missing values in datasets using matrix factorization approaches.

### ğŸ“˜ **[## Neural network example](./19_neural_net.ipynb)**

**Machine Learning Method Used:** Neural networks for supervised learning.

**Problem Addressed:** Building and training a simple neural network for predictive modeling.

![](/mnt/data/19_neural_net.png)

![](/mnt/data/19_neural_net.png)

![](/mnt/data/19_neural_net.png)

![](/mnt/data/19_neural_net.png)

### ğŸ“˜ **[---](./20_proximal_gradient_descent.ipynb)**

**Machine Learning Method Used:** Optimization using proximal gradient descent.

**Problem Addressed:** Optimizing complex functions with sparsity constraints in high-dimensional settings.

![](/mnt/data/20_proximal_gradient_descent.png)

![](/mnt/data/20_proximal_gradient_descent.png)

### ğŸ“˜ **[---](./21_proximal_gradient_descent_l1.ipynb)**

**Machine Learning Method Used:** L1 regularized optimization with proximal gradient descent.

**Problem Addressed:** Applying L1 regularization to encourage sparsity in machine learning models.

![](/mnt/data/21_proximal_gradient_descent_l1.png)

![](/mnt/data/21_proximal_gradient_descent_l1.png)

### ğŸ“˜ **[## Two neuron example](./22_two_neuron.ipynb)**

**Machine Learning Method Used:** Two-neuron network for understanding activation functions.

**Problem Addressed:** Exploring how different activation functions affect neural network behavior.

![](/mnt/data/22_two_neuron.png)

![](/mnt/data/22_two_neuron.png)

![](/mnt/data/22_two_neuron.png)

![](/mnt/data/22_two_neuron.png)

## ğŸ”¥ Skills Demonstrated
- **Kernel Methods & Support Vector Machines (SVM)**
- **LASSO Regression**
- **Matrix Methods in Machine Learning**
- **Optimization with Gradient Descent**
- **Principal Component Analysis (PCA)**
- **Singular Value Decomposition (SVD) & Eigenfaces**


## ğŸ“§ Contact
Interested in discussing potential opportunities? Feel free to connect!

ğŸ“¨ **Email:** your.email@example.com  
ğŸ’¼ **LinkedIn:** [Your LinkedIn](https://linkedin.com/in/yourprofile)  
ğŸŒ **Portfolio:** [Your Portfolio](https://yourwebsite.com)

---

*This portfolio showcases my ability to implement advanced matrix-based techniques in machine learning. Thank you for visiting!*
