{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Happy/ Angry Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__version__', '__globals__', 'y', 'X']\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "import sympy\n",
    "from sympy.abc import x\n",
    "\n",
    "in_data = loadmat('face_emotion_data.mat')\n",
    "print([key for key in in_data]) # -- use this line to see the keys in the dictionary data structure \n",
    "\n",
    "X = in_data['X']\n",
    "y = in_data['y']\n",
    "# print(np.shape(X))\n",
    "# print(np.shape(y))\n",
    "n_y = np.size(y)\n",
    "print(n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Use the training data X and y and a least squares problem to train your classiﬁer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94366942]\n",
      " [ 0.21373778]\n",
      " [ 0.26641775]\n",
      " [-0.39221373]\n",
      " [-0.00538552]\n",
      " [-0.01764687]\n",
      " [-0.16632809]\n",
      " [-0.0822838 ]\n",
      " [-0.16644364]]\n"
     ]
    }
   ],
   "source": [
    "# least squares as a loss function\n",
    "# w = (X^T X)^(-1)X^T y\n",
    "w = np.linalg.inv(X.transpose()@X)@X.transpose()@y\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Explain how to use the weights you found to classify a new face image as happy or angry?\n",
    "\n",
    "Utilize the weights to perform a dot product with the feature matrix of the face images (X). If the result is greater than 0, classify the image as +1 (happy); otherwise, classify it as -1 (angry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat = np.sign(X@w)\n",
    "# print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Which features seem to be most important? Justify your answer. Note that the nine columns of the training data feature matrix X have been normalized to have the same two-norm.\n",
    "\n",
    "The most crucial feature for determining whether an image is happy or not appears to be the 1st feature, as it possesses the highest weight value (0.94). Conversely, when discerning an angry face, the 4th feature becomes most significant, given its weight of (-0.39)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Design a classiﬁer based on three of the nine features. Which three should you choose? Describe the procedure for designing your classiﬁer.\r\n",
    "\r\n",
    "I created two classifier options:\r\n",
    "\r\n",
    "1. Selected features with the highest positive weights, along with the column with the greatest negative weight (indices: 0, 2, 3).\r\n",
    "2. Chose the column with the highest positive weight along with the first two columns featuring the greatest negative weights (indices: 0, 3, 8).\r\n",
    "\r\n",
    "\r\n",
    "I trained new classifiers for both options and compared their performance against each other and the original classifier. \r\n",
    "--> Features indexed 0, 3, and 8 were identified as the optimal choice based on the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate:2.34375%\n"
     ]
    }
   ],
   "source": [
    "# 9 feat performance\n",
    "error_vec = [0 if i[0]==i[1] else 1 for i in np.hstack((y_hat, y))]\n",
    "print('Error Rate:' + str(100*sum(error_vec)/ n_y) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94366942]\n",
      " [ 0.21373778]\n",
      " [ 0.26641775]\n",
      " [-0.39221373]\n",
      " [-0.00538552]\n",
      " [-0.01764687]\n",
      " [-0.16632809]\n",
      " [-0.0822838 ]\n",
      " [-0.16644364]]\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X)\n",
    "# option1: [0, 2, 3]\n",
    "X_1 = X[:, [0, 2, 3]]\n",
    "# print(X_1)\n",
    "# option2: [0, 3, 8]\n",
    "X_2 = X[:, [0, 3, 8]]\n",
    "# print(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate:6.25%\n"
     ]
    }
   ],
   "source": [
    "# option1: [0, 2, 3]\n",
    "w = np.linalg.inv(X_1.transpose()@X_1)@X_1.transpose()@y\n",
    "y_hat = np.sign(X_1@w)\n",
    "error_vec = [0 if i[0]==i[1] else 1 for i in np.hstack((y_hat, y))]\n",
    "print('Error Rate:' + str(100*sum(error_vec)/ n_y) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate:1.5625%\n"
     ]
    }
   ],
   "source": [
    "# option2: [0, 3, 8]\n",
    "w = np.linalg.inv(X_2.transpose()@X_2)@X_2.transpose()@y\n",
    "y_hat = np.sign(X_2@w)\n",
    "error_vec = [0 if i[0]==i[1] else 1 for i in np.hstack((y_hat, y))]\n",
    "print('Error Rate:' + str(100*sum(error_vec)/ n_y) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) What percent of the training labels are incorrectly classiﬁed using all nine features? What percent of the training labels are incorrectly classiﬁed using your reduced set of three features?\n",
    "\n",
    "Using all nine features, I observed a 2.34% misclassification rate. However, with the reduced set of three features, the misclassification rate improved to 1.56%, indicating better performance.\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Now use cross validation to assess your classiﬁer performance. Divide the available data in to eight subsets of sixteen samples (e.g., examples 1−16, 17−32, . . . , 113−128). Use seven sets to design your classiﬁer weights, then use the remaining holdout set to evaluate the classiﬁer performance. Compute the number of misclassiﬁcations made on this hold-out set and divide that number by 16 (the size of the set) to estimate the error rate for that hold-out set. Repeat this process eight times using the eight diﬀerent possible divisions between training and holdout sets and average the error rates to obtain a ﬁnal performance estimate.\n",
    "\n",
    "The ﬁnal performance estimate is 4.69%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Error Rate:4.6875%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# KFold: Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds\n",
    "# print(X[:16,:], y[:16])\n",
    "kf = KFold(n_splits=8)\n",
    "# for X_train, y_train, X_test, y_test in kf.split(X, y):\n",
    "total_error_rate = 0\n",
    "i = 0\n",
    "for train, test in kf.split(X):\n",
    "    # print(train, '\\n', test)\n",
    "    X_train = X[train, :]\n",
    "    y_train = y[train, :]\n",
    "    X_test = X[test, :]    \n",
    "    y_test = y[test, :]    \n",
    "    # print(X[test, :])\n",
    "    # print(y[test])\n",
    "    n_y_test = np.size(y_test)\n",
    "    # print(n_y_test)\n",
    "    w = np.linalg.inv(X_train.transpose()@X_train)@X_train.transpose()@y_train\n",
    "    y_hat = np.sign(X_test@w)\n",
    "    error_vec = [0 if i[0]==i[1] else 1 for i in np.hstack((y_hat, y_test))]\n",
    "    error_rate = 100*sum(error_vec)/ n_y_test\n",
    "    # print('Error Rate:' + str(error_rate) + '%')\n",
    "    i += 1\n",
    "    total_error_rate += error_rate\n",
    "    # print('Average Error Rate:' + str(total_error_rate/i) + '%')\n",
    "\n",
    "print('Average Error Rate:' + str(total_error_rate/i) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
